{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed9117a1-a2c7-49ff-a44b-6ddee7a4eeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "import functools\n",
    "import numpy as np\n",
    "import typing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7635bbb1-c7d5-4a35-93a7-b9a21076b996",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_METRICS = ['mic_f1', 'mac_f1']\n",
    "DEFAULT_CMP_BY = 'mic_f1'\n",
    "\n",
    "@functools.total_ordering\n",
    "class EvalResults(dict):\n",
    "    def __init__(self, mic_f1, mac_f1, cmp_by=DEFAULT_CMP_BY):\n",
    "        super().__init__({\n",
    "            'mic_f1': mic_f1,\n",
    "            'mac_f1': mac_f1\n",
    "        })\n",
    "        self.cmp_by = cmp_by\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if not isinstance(other, EvalResults):\n",
    "            return NotImplemented\n",
    "        fst = self.cmp_by\n",
    "        snd = [m for m in EVAL_METRICS if m != self.cmp_by][0]\n",
    "        if self[fst] < other[fst]:\n",
    "            return True\n",
    "        elif self[fst] > other[fst]:\n",
    "            return False\n",
    "        # self[fst] = other[fst]\n",
    "        elif self[snd] < other[snd]:\n",
    "            return True\n",
    "        else:\n",
    "            return False \n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, EvalResults):\n",
    "            return NotImplemented\n",
    "        # Equal if all metrics are the same\n",
    "        return all(self[m] == other[m] for m in EVAL_METRICS)\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self < other or self == other\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"mic_f1: {self['mic_f1']:.4f}, \"\n",
    "                f\"mac_f1: {self['mac_f1']:.4f}\")\n",
    "\n",
    "    def to_dict(self):\n",
    "        return { 'mic_f1': self['mic_f1'], 'mac_f1': self['mac_f1'] }\n",
    "\n",
    "    def to_serialisable(self):\n",
    "        return { k:str(v) for k,v in self.to_dict().items() }\n",
    "\n",
    "    @staticmethod\n",
    "    def average(results, std=True):\n",
    "        mic_f1s = [res['mic_f1'] for res in results]\n",
    "        mac_f1s = [res['mac_f1'] for res in results]\n",
    "\n",
    "        avgs = EvalResults(np.mean(mic_f1s), np.mean(mac_f1s))\n",
    "        stds = None if not std else EvalResults(np.std(mic_f1s), np.std(mac_f1s))\n",
    "        \n",
    "        return avgs, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979f3050-5c50-4aca-81bf-b0c4d4deda94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, mask, cmp_by=DEFAULT_CMP_BY):\n",
    "    \"\"\"\n",
    "    Evaluates model and returns its validation accuracy, \n",
    "    micro-F1 and macro-F1 scores on given mask.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # disable gradient computation during evaluation\n",
    "        # forward pass\n",
    "        out = model(data.x, data.edge_index)\n",
    "        # predict the class with max score\n",
    "        pred = out.argmax(dim=1)\n",
    "        true_labels = data.y[mask]\n",
    "        # calculate F1 scores (`f1_score` expects the inputs to be on the CPU)\n",
    "        mic_f1 = f1_score(true_labels.cpu(), pred[mask].cpu(), average='micro') # equivalent to accuracy for this task\n",
    "        mac_f1 = f1_score(true_labels.cpu(), pred[mask].cpu(), average='macro')\n",
    "\n",
    "    return EvalResults(mic_f1, mac_f1, cmp_by=cmp_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04171847-b3fb-49f2-bcb9-947873a37667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_training(params):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    data, dataset = load_data(params['dataset'], data_only=False)\n",
    "    params[\"n_classes\"] = dataset.num_classes  # number of target classes\n",
    "    params[\"input_dim\"] = dataset.num_features  # size of input features\n",
    "    \n",
    "    model = set_model(params, device)\n",
    "    model.param_init()\n",
    "    \n",
    "    optimiser = optim.Adam(model.parameters(), \n",
    "                           lr=params['lr'], \n",
    "                           weight_decay=params['weight_decay'])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return model, data, optimiser, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "178bb14c-43e1-4390-82dc-40a74592b8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_only(params: typing.Dict,\n",
    "               cmp_by=DEFAULT_CMP_BY,\n",
    "               report_per_period=1000,\n",
    "               print_results=True):\n",
    "    \"\"\"\n",
    "    Trains a node classification model and\n",
    "    returns the trained model object.\n",
    "    \"\"\"\n",
    "    model, data, optimiser, loss_fn = init_training(params)\n",
    "    n_epochs = params['epochs']\n",
    "\n",
    "    # variables for early stopping\n",
    "    best_results = EvalResults(-1,-1)  # best validation results\n",
    "    prev_loss = float('inf')\n",
    "    consec_worse_epochs = 0  # number of consecutive epochs with degrading results\n",
    "    # k: stop if epochs_dec_acc >= patience\n",
    "    patience = params['max_patience']\n",
    "\n",
    "    # standard training with backpropagation\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimiser.zero_grad()\n",
    "        out = model(data.x, data.edge_index) # forward pass\n",
    "        loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward() # backward pass\n",
    "        optimiser.step()\n",
    "\n",
    "        # evaluate on validation set\n",
    "        results = evaluate(model, data, data.val_mask, cmp_by=cmp_by)\n",
    "\n",
    "        # early stopping\n",
    "        if results >= best_results:\n",
    "            best_results = results\n",
    "            consec_worse_epochs = 0\n",
    "        else:\n",
    "            consec_worse_epochs += 1\n",
    "\n",
    "        # patience exceeded -> stop training\n",
    "        if consec_worse_epochs >= patience:\n",
    "            if print_results:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                print(f\"Best results: {best_results}\")\n",
    "            break\n",
    "\n",
    "        # print training progress\n",
    "        if (epoch+1) % report_per_period == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{n_epochs}...\")\n",
    "            print(f\"Loss: {loss};\")\n",
    "            print(f\"Validation Results:\\n{results}\\n\")\n",
    "\n",
    "    return model, best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a1a71f-2b56-4117-a3d1-658e1db4447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_tune(params, hyperparam, hyperparam_values, \n",
    "                   cmp_by=DEFAULT_CMP_BY,\n",
    "                   report_per_period=1000,\n",
    "                   print_results=True):\n",
    "    \"\"\"\n",
    "    Trains the model and performs hyperparameter tuning.\n",
    "\n",
    "    Args:\n",
    "    - params: Training parameters.\n",
    "    - hyperparam: Name of the hyperparameter to tune (if any).\n",
    "    - hyperparam_values: Values to test for the hyperparameter (if any).\n",
    "    - report_per_period: Frequency of training status reports.\n",
    "    - metric: Metric to optimise during training; one of [\"accuracy\", \"micro_f1\", or \"macro_f1\"].\n",
    "\n",
    "    Returns:\n",
    "    - if hyperparam_values = None, a pair: (trained model, its training performance) - same as [train]\n",
    "    - otherwise, a triple: (optimal trained model, ts training performance, its hyperparameter value)\n",
    "    \"\"\"\n",
    "    best_hyperparam_val, best_model = None, None, \n",
    "    best_results = EvalResults(-1,-1)\n",
    "\n",
    "    for val in hyperparam_values:\n",
    "        params[hyperparam] = val\n",
    "        model, results = train_only(params, cmp_by, \n",
    "                                    report_per_period, \n",
    "                                    print_results)\n",
    "        if results > best_results:\n",
    "            best_results = results\n",
    "            best_hyperparam_val = val\n",
    "            best_model = model\n",
    "\n",
    "    return best_model, best_results, best_hyperparam_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abb66d76-ca27-46cf-9d36-eaa2f10b6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params, hyperparam=None, \n",
    "          hyperparam_values=None, \n",
    "          cmp_by=DEFAULT_CMP_BY,\n",
    "          report_per_period=1000,\n",
    "          print_results=True):\n",
    "    \"\"\"\n",
    "    Wrapper training function.\n",
    "    Returns a triple: (model, training results, training hyperparameters)\n",
    "    \"\"\"\n",
    "    model_name = params['model_name']\n",
    "    if model_name not in MODEL_SPEC_HYPERPARAM: # train without tuning\n",
    "        model, res = train_only(params, cmp_by, report_per_period, print_results)\n",
    "        return model, res, params\n",
    "    else: \n",
    "        model, res, hyperparam_val = train_and_tune(params, hyperparam, \n",
    "                                                    hyperparam_values, \n",
    "                                                    cmp_by, report_per_period, \n",
    "                                                    print_results)\n",
    "        tuned_hyperparam = MODEL_SPEC_HYPERPARAM[model_name]\n",
    "        params[tuned_hyperparam] = hyperparam_val\n",
    "        return model, res, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f03d04b8-e4a0-45f5-8350-b0b1cc9d5023",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LAYERS = range(2,21,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3392272b-42d8-4910-a541-06a63a32cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diff_layers_model(params,\n",
    "                            layers=TRAIN_LAYERS,\n",
    "                            cmp_by=DEFAULT_CMP_BY,\n",
    "                            report_per_period=100,\n",
    "                            print_results=True):\n",
    "    model_name = params['model_name']\n",
    "    hyperparam = MODEL_SPEC_HYPERPARAM.get(model_name, None)\n",
    "    hyperparam_values = MODEL_HYPERPARAM_RANGE.get(model_name, None)\n",
    "    \n",
    "    layers_to_model = dict()\n",
    "    layers_to_hyperparams = dict()\n",
    "    for n in layers:\n",
    "        curr_params = params.copy()\n",
    "        curr_params['n_layers'] = n\n",
    "        model, res, hyperparams = train(curr_params, hyperparam, hyperparam_values, \n",
    "                                        cmp_by, report_per_period, print_results)\n",
    "        layers_to_model[n] =  model\n",
    "        layers_to_hyperparams[n] = hyperparams\n",
    "\n",
    "    return layers_to_model, layers_to_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63ad50-ff5a-4a4e-831b-38c833df4aeb",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8823a151-21ea-4ea1-a8f5-e7a41443fbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset_name):\n",
    "    data = load_data(dataset_name, data_only=True)\n",
    "    return evaluate(model, data, data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f566e5da-c45b-42dd-a35c-30e9f68cd1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(params,\n",
    "                   layers=TRAIN_LAYERS,\n",
    "                   cmp_by=DEFAULT_CMP_BY,\n",
    "                   report_per_period=1000,\n",
    "                   print_results=True,\n",
    "                   export_results=True):\n",
    "    dataset_name = params['dataset']\n",
    "    model_name = params['model_name']\n",
    "    layers_to_model, layers_to_hyperparams = train_diff_layers_model(params, \n",
    "                                                                     layers, \n",
    "                                                                     cmp_by,\n",
    "                                                                     report_per_period,\n",
    "                                                                     print_results)\n",
    "    layers_to_results = dict()  # number of layers : test results\n",
    "    for n, model in layers_to_model.items():\n",
    "        layers_to_results[n] = test(model, dataset_name)\n",
    "        layers_to_model[n] = model\n",
    "        if export_results:\n",
    "            torch.save(model.state_dict(), f'./results/{dataset_name}/{n}_layers.pt')\n",
    "\n",
    "    if print_results:\n",
    "        print(f\"Test results for {model_name} on {dataset_name}:\")\n",
    "        for n, results in layers_to_results.items():\n",
    "            print(f\"{n}-layer model:\")\n",
    "            print(f\"Hyperparameter setting:\")\n",
    "            print(layers_to_hyperparams[n])\n",
    "            print(results)\n",
    "        print()\n",
    "    if export_results:\n",
    "        with open(f'./results/{dataset_name}/results.json', 'w') as fp:\n",
    "            data = {n : metrics.to_serialisable() for n,metrics in layers_to_results.items() }\n",
    "            json.dump(data, fp, indent=4)\n",
    "\n",
    "    return layers_to_model, layers_to_results, layers_to_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee23c829-a01d-4ae4-ba9f-e9d889be836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stderr(xs):\n",
    "    return np.std(xs, ddof=1) / np.sqrt(len(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb29d12b-f6b3-43f6-b64f-3e8503c78962",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TRAINING_PARAMS = {\n",
    "    \"lr\": 0.01,  # learning rate\n",
    "    \"weight_decay\": 0.0005,  # weight_decay\n",
    "    \"epochs\": 400,  # number of total training epochs\n",
    "    \"max_patience\": 5, # number of k for early stopping\n",
    "    \"hid_dim\": 64, # hidden dimensions\n",
    "    \"init_res_weight\": 0\n",
    "}\n",
    "\n",
    "def training_params(model_name, dataset_name, init_res_weight=0, n_layers=2):\n",
    "    params = DEFAULT_TRAINING_PARAMS.copy()\n",
    "    params['model_name'] = model_name\n",
    "    params['dataset'] = dataset_name\n",
    "    params['init_res_weight'] = init_res_weight\n",
    "    params['n_layers'] = n_layers\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5cab5e-0de7-443a-b877-3fe7ae4e8ece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
