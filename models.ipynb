{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a3e3ef-0c56-4dc5-aafb-6a43ce0ba6ad",
   "metadata": {},
   "source": [
    "# Models\n",
    "Here we set up graph convolutional networks for node classification.\n",
    "All our GCN class takes five inputs: *input_dim*, *hid_dim*, *n_class*, *n_layers*, and *dropout_ratio*.\n",
    "\n",
    "- The `forward` function should return a Tensor object: **logits**\n",
    "- The `generate_node_embeddings` fuction should return a Tensor object: **node_embeddings**, which is the representation of the last layer.\n",
    "- We use `F.relu` and `F.dropout` at the end of each layer.\n",
    "- We assume all models will have at least 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc5284ef-b50c-4c9f-b9c1-1c5aceee3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import typing\n",
    "import torch_geometric\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.datasets as datasets\n",
    "\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c37c9d98-e4d9-4b6c-9675-180e914fb468",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = ['GCN', 'SkipGCN', 'DropEdgeGCN', 'JumpKnowGCN', 'WeightedSkipGCN', 'WeightedSkipDropGCN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c84fdccb-cb1c-4234-a6ef-f2761a274b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hid_dim: int,\n",
    "                 n_classes: int, n_layers: int,\n",
    "                 act_fn = F.relu):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: input feature dimension\n",
    "            hid_dim: hidden feature dimension\n",
    "            n_classes: number of target classes\n",
    "            n_layers: number of layers\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        # assert n_layers > 1\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes) # final MLP for generating logits\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "        self.param_init()\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        X = self._forward_before_final_layer(X, A)\n",
    "        return self.mlp(X)  # do not apply non-linearity before MLP\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        \"\"\" Generate node embeddings without applying the MLP. \"\"\"\n",
    "        X = self._forward_before_final_layer(X, A)\n",
    "        X = self.act_fn(X)\n",
    "        return X  # raw GNN output without applying MLP\n",
    "\n",
    "    def _forward_before_final_layer(self, X, A) -> torch.Tensor:\n",
    "        \"\"\" Apply all layers except for the final layer and MLP. \"\"\"\n",
    "        for l in self.layers[:-1]: # message-passing through all layers except for the last\n",
    "            X = l(X, A)\n",
    "            X = self.act_fn(X)\n",
    "        return X  # raw GNN output without applying MLP\n",
    "\n",
    "    def param_init(self):\n",
    "        # initialise MLP parameters\n",
    "        nn.init.xavier_uniform_(self.mlp.weight)\n",
    "        nn.init.zeros_(self.mlp.bias)\n",
    "        for conv in self.layers:\n",
    "            # initialise weight in each layer's Linear object\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "91bc0acd-bc85-44dc-a49d-b5685872ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropEdgeGCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hid_dim: int,\n",
    "                 n_classes: int, n_layers: int,\n",
    "                 dropout_ratio: float = 0.3,\n",
    "                 act_fn = F.relu):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: input feature dimension\n",
    "            hid_dim: hidden feature dimension\n",
    "            n_classes: number of target classes\n",
    "            n_layers: number of layers\n",
    "            dropout_ratio: dropout_ratio\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        # assert n_layers > 1\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes) # final MLP for generating logits\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "        self.param_init()\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        X = self._forward_before_final_layer(X, A)\n",
    "        return self.mlp(X)  # do not apply non-linearity & dropout before MLP\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        \"\"\" Generate node embeddings without applying the MLP. \"\"\"\n",
    "        X = self._forward_before_final_layer(X, A)\n",
    "        X = self.act_fn(X)\n",
    "        X = F.dropout(X, p=self.dropout_ratio, training=self.training)\n",
    "        return X  # raw GNN output without applying MLP\n",
    "\n",
    "    def _forward_before_final_layer(self, X, A) -> torch.Tensor:\n",
    "        \"\"\" Apply all layers except for the final layer and MLP. \"\"\"\n",
    "        for l in self.layers[:-1]: # message-passing through all layers except for the last\n",
    "            X = l(X, A)\n",
    "            X = self.act_fn(X)\n",
    "            X = F.dropout(X, p=self.dropout_ratio, training=self.training)\n",
    "        return X  # raw GNN output without applying MLP\n",
    "\n",
    "    def param_init(self):\n",
    "        # initialise MLP parameters\n",
    "        nn.init.xavier_uniform_(self.mlp.weight)\n",
    "        nn.init.zeros_(self.mlp.bias)\n",
    "        for conv in self.layers:\n",
    "            # initialise weight in each layer's Linear object\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d2e7db2-3a51-4e78-8301-6359d6b827cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hid_dim: int,\n",
    "                 n_classes: int, n_layers: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_dim: input feature dimension\n",
    "          hid_dim: hidden feature dimension\n",
    "          n_classes: number of target classes\n",
    "          n_layers: number of layers\n",
    "        \"\"\"\n",
    "        super(SkipGCN, self).__init__()\n",
    "        assert n_layers > 1\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes)\n",
    "\n",
    "        self.param_init()\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        X = self.generate_node_embeddings(X, A)\n",
    "        return self.mlp(X)\n",
    "\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        \"\"\"Generate node embeddings without applying the MLP.\"\"\"\n",
    "        X = self.layers[0](X, A)\n",
    "        X = F.relu(X)\n",
    "\n",
    "        for l in self.layers[1:-1]: # message-passing through all layers\n",
    "            residual = X  # previous layer's representation\n",
    "            X = l(X, A)\n",
    "            X = F.relu(X)\n",
    "            X = X + residual  # add skip connection\n",
    "\n",
    "        return self.layers[-1](X, A)\n",
    "\n",
    "    def param_init(self):\n",
    "        nn.init.xavier_uniform_(self.mlp.weight)\n",
    "        nn.init.zeros_(self.mlp.bias)\n",
    "        for conv in self.layers:\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "92677214-47d7-4577-a54f-29ce9a50043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JumpKnowGCN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hid_dim: int,\n",
    "        n_classes: int,\n",
    "        n_layers: int,\n",
    "        dropout_ratio: float = 0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: input feature dimension\n",
    "            hid_dim: hidden feature dimension\n",
    "            n_classes: number of target classes\n",
    "            n_layers: number of layers\n",
    "        \"\"\"\n",
    "        super(JumpKnowGCN, self).__init__()\n",
    "        assert n_layers > 1\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes)\n",
    "\n",
    "        self.param_init()\n",
    "\n",
    "\n",
    "    def _layer_outputs(self, X, A) -> torch.Tensor:\n",
    "        \"\"\" Outputs of all layers\n",
    "            (no activation applied to the final layer -\n",
    "            i.e. last element is just logits)\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        for l in self.layers[:-1]:\n",
    "            X = l(X, A)\n",
    "            X = F.relu(X)\n",
    "            outputs.append(X)\n",
    "\n",
    "        outputs.append(self.layers[-1](X, A))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "      return self._layer_outputs(X, A)[-1]\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        outputs = self._layer_outputs(X, A)[:-1]\n",
    "        return torch.max(torch.stack(outputs), 0)[0]  # max pooling\n",
    "\n",
    "    def param_init(self):\n",
    "        for conv in self.layers:\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "457e5126-82a5-4a71-a3a2-971e9050815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSkipGCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hid_dim: int,\n",
    "                 n_classes: int, n_layers: int,\n",
    "                 init_res_weight: float = 0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_dim: input feature dimension\n",
    "          hid_dim: hidden feature dimension\n",
    "          n_classes: number of target classes\n",
    "          n_layers: number of layers\n",
    "        \"\"\"\n",
    "        super(WeightedSkipGCN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.res_weight = nn.Parameter(torch.tensor(init_res_weight,\n",
    "                                                    dtype=torch.float32))\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes)\n",
    "        self.param_init()\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        X = self.generate_node_embeddings(X, A)\n",
    "        print(\"current residual weight:\", self.res_weight)\n",
    "        return self.mlp(X)  # MLP maps to logits\n",
    "\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        \"\"\"Generate node embeddings without applying the MLP.\"\"\"\n",
    "        # First GCNConv layer\n",
    "        X = self.layers[0](X, A)\n",
    "        X = F.relu(X)\n",
    "\n",
    "        # Intermediate GCNConv layers with skip connections\n",
    "        for l in self.layers[1:-1]:\n",
    "            residual = X  # previous layer's representation\n",
    "            X = l(X, A)\n",
    "            X = F.relu(X)\n",
    "            X = self.res_weight * X + residual  # Add skip connection\n",
    "\n",
    "        # Final GCNConv layer outputs raw embeddings\n",
    "        return self.layers[-1](X, A)\n",
    "\n",
    "    def param_init(self):\n",
    "        nn.init.xavier_uniform_(self.mlp.weight)\n",
    "        nn.init.zeros_(self.mlp.bias)\n",
    "        for conv in self.layers:\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a345722b-e9aa-4eb7-98d7-f50537369534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSkipDropGCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hid_dim: int,\n",
    "                 n_classes: int, n_layers: int,\n",
    "                 dropout_ratio: float = 0.3,\n",
    "                 init_res_weight: float = 0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_dim: input feature dimension\n",
    "          hid_dim: hidden feature dimension\n",
    "          n_classes: number of target classes\n",
    "          n_layers: number of layers\n",
    "          dropout_ratio: dropout ratio\n",
    "        \"\"\"\n",
    "        super(WeightedSkipGCN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.res_weight = nn.Parameter(torch.tensor(init_res_weight,\n",
    "                                                    dtype=torch.float32))\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes)\n",
    "        self.param_init()\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        X = self.generate_node_embeddings(X, A)\n",
    "        print(\"current residual weight:\", self.res_weight)\n",
    "        return self.mlp(X)  # MLP maps to logits\n",
    "\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        \"\"\"Generate node embeddings without applying the MLP.\"\"\"\n",
    "        # First GCNConv layer\n",
    "        X = self.layers[0](X, A)\n",
    "        X = F.relu(X)\n",
    "        X = F.dropout(X, p=self.dropout_ratio, training=self.training)\n",
    "\n",
    "        # Intermediate GCNConv layers with skip connections\n",
    "        for l in self.layers[1:-1]:\n",
    "            residual = X  # previous layer's representation\n",
    "            X = l(X, A)\n",
    "            X = F.relu(X)\n",
    "            dropout_mask = F.dropout(\n",
    "                torch.ones_like(X),\n",
    "                p=self.dropout_ratio,\n",
    "                training=self.training)\n",
    "            X = X * dropout_mask\n",
    "            residual = residual * dropout_mask\n",
    "            X = self.res_weight * X + residual  # Add skip connection\n",
    "\n",
    "        # Final GCNConv layer outputs raw embeddings\n",
    "        return self.layers[-1](X, A)\n",
    "\n",
    "    def param_init(self):\n",
    "        nn.init.xavier_uniform_(self.mlp.weight)\n",
    "        nn.init.zeros_(self.mlp.bias)\n",
    "        for conv in self.layers:\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9daed07d-f469-4125-b824-d2218f943623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(params, device):\n",
    "    \"\"\"\n",
    "    Returns the model initialised based on the configuration specified by `params`.\n",
    "    \"\"\"\n",
    "    model_name = params['model_name']\n",
    "    model_params = [params[\"input_dim\"], params[\"hid_dim\"],\n",
    "                    params[\"n_classes\"], params[\"n_layers\"], params['dropout_ratio']]\n",
    "    if model_name == 'WeightedSkipGCN':\n",
    "        return WeightedSkipGCN(*model_params, init_res_weight=params['init_res_weight'])\n",
    "    elif model_name in MODELS:\n",
    "        return globals()[model_name](*model_params)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2373ca4-a132-4b83-9b79-f25c1f65a292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
