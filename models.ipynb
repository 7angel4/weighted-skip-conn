{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5284ef-b50c-4c9f-b9c1-1c5aceee3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import typing\n",
    "import torch_geometric\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.datasets as datasets\n",
    "\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84fdccb-cb1c-4234-a6ef-f2761a274b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hid_dim: int,\n",
    "                 n_classes: int, n_layers: int,\n",
    "                 dropout_ratio: float = 0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: input feature dimension\n",
    "            hid_dim: hidden feature dimension\n",
    "            n_classes: number of target classes\n",
    "            n_layers: number of layers\n",
    "            dropout_ratio: dropout_ratio\n",
    "        \"\"\"\n",
    "        super(GCN, self).__init__()\n",
    "        assert n_layers > 1\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes) # final MLP for generating logits\n",
    "\n",
    "        self.param_init()\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        X = self.generate_node_embeddings(X, A)\n",
    "        return self.mlp(X)\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        \"\"\" Generate node embeddings without applying the MLP. \"\"\"\n",
    "        for l in self.layers[:-1]: # message-passing through all layers\n",
    "            X = l(X, A)\n",
    "            X = F.relu(X)\n",
    "            X = F.dropout(X, p=self.dropout_ratio, training=self.training)\n",
    "        return self.layers[-1](X, A)  # raw GNN output without applying MLP\n",
    "\n",
    "    def param_init(self):\n",
    "        # initialise MLP parameters\n",
    "        nn.init.xavier_uniform_(self.mlp.weight)\n",
    "        nn.init.zeros_(self.mlp.bias)\n",
    "        for conv in self.layers:\n",
    "            # initialise weight in each layer's Linear object\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e7db2-3a51-4e78-8301-6359d6b827cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hid_dim: int,\n",
    "                 n_classes: int, n_layers: int,\n",
    "                 dropout_ratio: float = 0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_dim: input feature dimension\n",
    "          hid_dim: hidden feature dimension\n",
    "          n_classes: number of target classes\n",
    "          n_layers: number of layers\n",
    "          dropout_ratio: dropout ratio\n",
    "        \"\"\"\n",
    "        super(SkipGCN, self).__init__()\n",
    "        assert n_layers > 1\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes)\n",
    "\n",
    "        self.param_init()\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        X = self.generate_node_embeddings(X, A)\n",
    "        return self.mlp(X)\n",
    "\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        \"\"\"Generate node embeddings without applying the MLP.\"\"\"\n",
    "        X = self.layers[0](X, A)\n",
    "        X = F.relu(X)\n",
    "        X = F.dropout(X, p=self.dropout_ratio, training=self.training)\n",
    "\n",
    "        for l in self.layers[1:-1]: # message-passing through all layers\n",
    "            residual = X  # previous layer's representation\n",
    "            X = l(X, A)\n",
    "            X = F.relu(X)\n",
    "            dropout_mask = F.dropout(\n",
    "                torch.ones_like(X),\n",
    "                p=self.dropout_ratio,\n",
    "                training=self.training)\n",
    "            X = X * dropout_mask\n",
    "            residual = residual * dropout_mask\n",
    "            X = X + residual  # add skip connection\n",
    "\n",
    "        return self.layers[-1](X, A)\n",
    "\n",
    "    def param_init(self):\n",
    "        nn.init.xavier_uniform_(self.mlp.weight)\n",
    "        nn.init.zeros_(self.mlp.bias)\n",
    "        for conv in self.layers:\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92677214-47d7-4577-a54f-29ce9a50043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JumpKnowGCN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hid_dim: int,\n",
    "        n_classes: int,\n",
    "        n_layers: int,\n",
    "        dropout_ratio: float = 0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: input feature dimension\n",
    "            hid_dim: hidden feature dimension\n",
    "            n_classes: number of target classes\n",
    "            n_layers: number of layers\n",
    "            dropout_ratio: dropout ratio\n",
    "        \"\"\"\n",
    "        super(JumpKnowGCN, self).__init__()\n",
    "        assert n_layers > 1\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes)\n",
    "\n",
    "        self.param_init()\n",
    "\n",
    "\n",
    "    def _layer_outputs(self, X, A) -> torch.Tensor:\n",
    "        \"\"\" Outputs of all layers\n",
    "            (no activation & dropout applied to the final layer -\n",
    "            i.e. last element is just logits)\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        for l in self.layers[:-1]:\n",
    "            X = l(X, A)\n",
    "            X = F.relu(X)\n",
    "            X = F.dropout(X, p=self.dropout_ratio, training=self.training)\n",
    "            outputs.append(X)\n",
    "\n",
    "        outputs.append(self.layers[-1](X, A))\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "      return self._layer_outputs(X, A)[-1]\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        outputs = self._layer_outputs(X, A)[:-1]\n",
    "        return torch.max(torch.stack(outputs), 0)[0]  # max pooling\n",
    "\n",
    "    def param_init(self):\n",
    "        for conv in self.layers:\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a345722b-e9aa-4eb7-98d7-f50537369534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSkipGCN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hid_dim: int,\n",
    "                 n_classes: int, n_layers: int,\n",
    "                 dropout_ratio: float = 0.3,\n",
    "                 init_res_weight: float = 0.3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_dim: input feature dimension\n",
    "          hid_dim: hidden feature dimension\n",
    "          n_classes: number of target classes\n",
    "          n_layers: number of layers\n",
    "          dropout_ratio: dropout ratio\n",
    "        \"\"\"\n",
    "        super(WeightedSkipGCN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.res_weight = nn.Parameter(torch.tensor(init_res_weight,\n",
    "                                                    dtype=torch.float32))\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes)\n",
    "        self.param_init()\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        X = self.generate_node_embeddings(X, A)\n",
    "        return self.mlp(X)  # MLP maps to logits\n",
    "\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        \"\"\"Generate node embeddings without applying the MLP.\"\"\"\n",
    "        # First GCNConv layer\n",
    "        X = self.layers[0](X, A)\n",
    "        X = F.relu(X)\n",
    "        X = F.dropout(X, p=self.dropout_ratio, training=self.training)\n",
    "\n",
    "        # Intermediate GCNConv layers with skip connections\n",
    "        for l in self.layers[1:-1]:\n",
    "            residual = X  # previous layer's representation\n",
    "            X = l(X, A)\n",
    "            X = F.relu(X)\n",
    "            dropout_mask = F.dropout(\n",
    "                torch.ones_like(X),\n",
    "                p=self.dropout_ratio,\n",
    "                training=self.training)\n",
    "            X = X * dropout_mask\n",
    "            residual = residual * dropout_mask\n",
    "            X = self.res_weight * X + residual  # Add skip connection\n",
    "\n",
    "        # Final GCNConv layer outputs raw embeddings\n",
    "        return self.layers[-1](X, A)\n",
    "\n",
    "    def param_init(self):\n",
    "        nn.init.xavier_uniform_(self.mlp.weight)\n",
    "        nn.init.zeros_(self.mlp.bias)\n",
    "        for conv in self.layers:\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
