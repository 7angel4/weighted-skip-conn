{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/7angel4/weighted-jk/blob/main/JKGCN_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3FZwh069JY5"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iz23VzjZUVa-"
   },
   "source": [
    "## 0. Set up dependencies\n",
    "Run the following blocks of code to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLY4MpqHjTok",
    "outputId": "b1a092cc-251f-46b1-f2f7-166217e1f8b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0.dev20241112\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "02LZGDSPE4c6"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip uninstall torch-scatter torch-sparse torch-geometric torch-cluster  --y\n",
    "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
    "!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
    "!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.1+cu118.html\n",
    "!pip install git+https://github.com/pyg-team/pytorch_geometric.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress warnings\n",
    "import warnings\n",
    "\n",
    "# Suppress specific UserWarning related to InMemoryDataset\n",
    "warnings.simplefilter(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjTN3_eBVEN0"
   },
   "source": [
    "## 1. Oversmoothing\n",
    "\n",
    "We aim to investigate **oversmoothing**: we will consider different number of layers and visualise the corresponding node embeddings. Specifically, we will experiment using the following dataset and methods::\n",
    "\n",
    "1.   Dataset: CORA\n",
    "2.   Model: Graph Convolutional Network\n",
    "3.   Dimensionality reduction method: T-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Slw4kPEfXEF-"
   },
   "source": [
    "#### Import required packages\n",
    "\n",
    "Please run the below code block to import the required python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U8C0pQUR6e2P",
    "outputId": "00e41feb-b0e5-46fe-fd48-7d8e6cb9878b"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import typing\n",
    "import torch_geometric\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.datasets as datasets\n",
    "\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1070a0230>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(77888) # set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run datasets.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run models.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaHpr42SrxaX"
   },
   "source": [
    "## 2. `train` and `evaluate` functions for training a node classification model.\n",
    "\n",
    "- Apply **Early-stopping**: stop training if the validation accuracy decreases during *k* consecutive epochs.\n",
    "- Use **Adam optimizer** for training.\n",
    "- Use **evaluate** function for calculating the validation accuracy in every epoch to adapt early-stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_METRICS = ['accuracy', 'micro_f1', 'macro_f1']\n",
    "DEFAULT_CMP_BY = ['accuracy', 'micro_f1', 'macro_f1']\n",
    "\n",
    "@functools.total_ordering\n",
    "class EvalResults(dict):\n",
    "    def __init__(self, acc, micro_f1, macro_f1, cmp_by=DEFAULT_CMP_BY):\n",
    "        # Initialize EvalResults with metrics as dictionary keys\n",
    "        super().__init__({\n",
    "            'accuracy': acc,\n",
    "            'micro_f1': micro_f1,\n",
    "            'macro_f1': macro_f1\n",
    "        })\n",
    "        # Ensure all comparison metrics are valid\n",
    "        assert all(m in EVAL_METRICS for m in cmp_by), \\\n",
    "            f\"All metrics in cmp_by must be one of {EVAL_METRICS}\"\n",
    "        self.cmp_by = cmp_by\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if not isinstance(other, EvalResults):\n",
    "            return NotImplemented\n",
    "        # Compare in the order specified by cmp_by\n",
    "        for metric in self.cmp_by:\n",
    "            if self[metric] < other[metric]:\n",
    "                return True\n",
    "            elif self[metric] > other[metric]:\n",
    "                return False\n",
    "        return False  # Equal in all specified metrics\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, EvalResults):\n",
    "            return NotImplemented\n",
    "        # Equal if all metrics are the same\n",
    "        return all(self[metric] == other[metric] for metric in self.cmp_by)\n",
    "\n",
    "    def __le__(self, other):\n",
    "        return self < other or self == other\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"accuracy: {self['accuracy']:.4f}, \"\n",
    "                f\"micro_f1: {self['micro_f1']:.4f}, \"\n",
    "                f\"macro_f1: {self['macro_f1']:.4f}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def average(results, std=True):\n",
    "        \"\"\"\n",
    "        Calculates the average and standard deviation of the metrics \n",
    "        across a list of EvalResults instances.\n",
    "\n",
    "        Args:\n",
    "            results (list): A list of EvalResults instances.\n",
    "\n",
    "        Returns: a pair of dictionaries, respectively containing the \n",
    "                 mean and standard deviation of each metric.\n",
    "        \"\"\"\n",
    "        accuracies = [res['accuracy'] for res in results]\n",
    "        micro_f1s = [res['micro_f1'] for res in results]\n",
    "        macro_f1s = [res['macro_f1'] for res in results]\n",
    "\n",
    "        avgs = EvalResults(np.mean(accuracies), np.mean(micro_f1s), np.mean(macro_f1s))\n",
    "        stds = None if not std else \\\n",
    "                  EvalResults(np.std(accuracies), np.std(micro_f1s), np.std(macro_f1s))\n",
    "        \n",
    "        return avgs, stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, mask, cmp_by=DEFAULT_CMP_BY):\n",
    "    \"\"\"\n",
    "    Evaluates model and returns its validation accuracy, \n",
    "    micro-F1 and macro-F1 scores on given mask.\n",
    "    \"\"\"\n",
    "    model.eval()  # set to evaluation mode\n",
    "    with torch.no_grad():  # disable gradient computation during evaluation\n",
    "        # forward pass\n",
    "        out = model(data.x, data.edge_index)\n",
    "        # predict the class with max score\n",
    "        pred = out.argmax(dim=1)\n",
    "        true_labels = data.y[mask]\n",
    "        \n",
    "        # calculate accuracy\n",
    "        correct = pred[mask] == true_labels\n",
    "        accuracy = correct.sum() / mask.sum()\n",
    "\n",
    "        # calculate F1 scores (`f1_score` expects the inputs to be on the CPU)\n",
    "        micro_f1 = f1_score(true_labels.cpu(), pred[mask].cpu(), average='micro')\n",
    "        macro_f1 = f1_score(true_labels.cpu(), pred[mask].cpu(), average='macro')\n",
    "\n",
    "    return EvalResults(accuracy, micro_f1, macro_f1, cmp_by=cmp_by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_training(params):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    data, dataset = load_data(params['dataset'], data_only=False)\n",
    "    params[\"n_classes\"] = dataset.num_classes  # number of target classes\n",
    "    params[\"input_dim\"] = dataset.num_features  # size of input features\n",
    "    \n",
    "    model = set_model(params, device)\n",
    "    model.param_init()\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                           lr=params['lr'], \n",
    "                           weight_decay=params['weight_decay'])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    \n",
    "    return model, data, optimizer, loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "7zehjgPztuYC"
   },
   "outputs": [],
   "source": [
    "def train_only(params: typing.Dict,\n",
    "               cmp_by=DEFAULT_CMP_BY,\n",
    "               report_per_period=1000,\n",
    "               print_results=True):\n",
    "    \"\"\"\n",
    "    Trains a node classification model and\n",
    "    returns the trained model object.\n",
    "    \"\"\"\n",
    "    model, data, optimizer, loss_fn = init_training(params)\n",
    "    n_epochs = params['epochs']\n",
    "\n",
    "    # variables for early stopping\n",
    "    best_results = EvalResults(-1,-1,-1)  # best validation results\n",
    "    prev_loss = float('inf')\n",
    "    consec_worse_epochs = 0  # number of consecutive epochs with degrading results\n",
    "    # k: stop if epochs_dec_acc >= patience\n",
    "    patience = params['max_patience']\n",
    "\n",
    "    # standard training with backpropagation\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index) # forward pass\n",
    "        loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "        loss.backward() # backward pass\n",
    "        optimizer.step()\n",
    "\n",
    "        # evaluate on validation set\n",
    "        results = evaluate(model, data, data.val_mask, cmp_by=cmp_by)\n",
    "\n",
    "        # early stopping\n",
    "        if results >= best_results:\n",
    "            best_results = results\n",
    "            consec_worse_epochs = 0\n",
    "        else:\n",
    "            consec_worse_epochs += 1\n",
    "\n",
    "        # patience exceeded -> stop training\n",
    "        if consec_worse_epochs >= patience:\n",
    "            if print_results:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                print(f\"Best results: {best_results}\")\n",
    "            break\n",
    "\n",
    "        # print training progress\n",
    "        if (epoch+1) % report_per_period == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{n_epochs}...\")\n",
    "            print(f\"Loss: {loss};\")\n",
    "            print(f\"Validation Results:\\n{results}\\n\")\n",
    "\n",
    "    return model, best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_params = {\n",
    "    \"lr\": 0.01,  # learning rate\n",
    "    \"weight_decay\": 0.0005,  # weight_decay\n",
    "    \"epochs\": 1000,  # number of total training epochs\n",
    "    \"max_patience\": 5, # number of k for early stopping\n",
    "    \"hid_dim\": 64, # size of hidden features\n",
    "    \"model_name\": \"PlainGNN\",\n",
    "    \"n_layers\": 2,\n",
    "    \"dataset\": 'Cora'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_tune(params, hyperparam, hyperparam_values, \n",
    "                   cmp_by=DEFAULT_CMP_BY,\n",
    "                   report_per_period=1000,\n",
    "                   print_results=True):\n",
    "    \"\"\"\n",
    "    Trains the model and performs hyperparameter tuning.\n",
    "\n",
    "    Args:\n",
    "    - params: Training parameters.\n",
    "    - hyperparam: Name of the hyperparameter to tune (if any).\n",
    "    - hyperparam_values: Values to test for the hyperparameter (if any).\n",
    "    - report_per_period: Frequency of training status reports.\n",
    "    - metric: Metric to optimise during training; one of [\"accuracy\", \"micro_f1\", or \"macro_f1\"].\n",
    "\n",
    "    Returns:\n",
    "    - if hyperparam_values = None, a pair: (trained model, its training performance) - same as [train]\n",
    "    - otherwise, a triple: (optimal trained model, ts training performance, its hyperparameter value)\n",
    "    \"\"\"\n",
    "    best_hyperparam_val, best_model = None, None, \n",
    "    best_results = EvalResults(-1,-1,-1)\n",
    "\n",
    "    for val in hyperparam_values:\n",
    "        params[hyperparam] = val\n",
    "        model, results = train_only(params, cmp_by, \n",
    "                                    report_per_period, \n",
    "                                    print_results)\n",
    "        if results > best_results:\n",
    "            best_results = results\n",
    "            best_hyperparam_val = val\n",
    "            best_model = model\n",
    "\n",
    "    return best_model, best_results, best_hyperparam_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(params, hyperparam=None, \n",
    "          hyperparam_values=None, \n",
    "          cmp_by=DEFAULT_CMP_BY,\n",
    "          report_per_period=1000,\n",
    "          print_results=True):\n",
    "    \"\"\"\n",
    "    Wrapper training function.\n",
    "    Returns a triple: (model, training results, training hyperparameters)\n",
    "    \"\"\"\n",
    "    model_name = params['model_name']\n",
    "    if model_name not in MODEL_SPEC_HYPERPARAM: # train without tuning\n",
    "        model, res = train_only(params, cmp_by, report_per_period, print_results)\n",
    "        return model, res, params\n",
    "    else: \n",
    "        model, res, hyperparam_val = train_and_tune(params, hyperparam, \n",
    "                                                    hyperparam_values, \n",
    "                                                    cmp_by, report_per_period, \n",
    "                                                    print_results)\n",
    "        tuned_hyperparam = MODEL_SPEC_HYPERPARAM[model_name]\n",
    "        params[tuned_hyperparam] = hyperparam_val\n",
    "        return model, res, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LAYERS = range(2,21,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2cL3tLKyn-r"
   },
   "source": [
    "#### 3.1. Train 4 GCN models with varying number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "fVkKx3B369Eh"
   },
   "outputs": [],
   "source": [
    "def train_diff_layers_model(params,\n",
    "                            layers=TRAIN_LAYERS,\n",
    "                            cmp_by=DEFAULT_CMP_BY,\n",
    "                            report_per_period=100,\n",
    "                            print_results=True):\n",
    "    model_name = params['model_name']\n",
    "    hyperparam = MODEL_SPEC_HYPERPARAM.get(model_name, None)\n",
    "    hyperparam_values = MODEL_HYPERPARAM_RANGE.get(model_name, None)\n",
    "    \n",
    "    layers_to_model = dict()\n",
    "    layers_to_hyperparams = dict()\n",
    "    for n in layers:\n",
    "        curr_params = params.copy()\n",
    "        curr_params['n_layers'] = n\n",
    "        model, res, hyperparams = train(curr_params, hyperparam, hyperparam_values, \n",
    "                                        cmp_by, report_per_period, print_results)\n",
    "        layers_to_model[n] =  model\n",
    "        layers_to_hyperparams[n] = hyperparams\n",
    "\n",
    "    return layers_to_model, layers_to_hyperparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataset_name):\n",
    "    data = load_data(dataset_name, data_only=True)\n",
    "    return evaluate(model, data, data.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(params,\n",
    "                   layers=TRAIN_LAYERS,\n",
    "                   cmp_by=DEFAULT_CMP_BY,\n",
    "                   report_per_period=1000,\n",
    "                   print_results=True,\n",
    "                   export_results=True):\n",
    "    dataset_name = params['dataset']\n",
    "    model_name = params['model_name']\n",
    "    layers_to_model, layers_to_hyperparams = train_diff_layers_model(params, \n",
    "                                                                     layers, \n",
    "                                                                     cmp_by,\n",
    "                                                                     report_per_period,\n",
    "                                                                     print_results)\n",
    "    layers_to_results = dict()  # number of layers : test results\n",
    "    for n, model in layers_to_model.items():\n",
    "        layers_to_results[n] = test(model, dataset_name)\n",
    "\n",
    "    if print_results:\n",
    "        print(f\"Test results for {model_name} on {dataset_name}:\")\n",
    "        for n, results in layers_to_results.items():\n",
    "            print(f\"{n}-layer model:\")\n",
    "            print(f\"Hyperparameter setting:\")\n",
    "            print(layers_to_hyperparams[n])\n",
    "            print(results)\n",
    "        print()\n",
    "\n",
    "    if export_results:\n",
    "        export_dir = f\"results/{dataset_name}/{model_name}/\"      \n",
    "        # export model as .pt files\n",
    "        for n, model in layers_to_model.items():\n",
    "            model_path = export_dir + f\"{n}_layers_model.pt\"\n",
    "            torch.save(model, model_path)\n",
    "        \n",
    "        # Save results\n",
    "        results_path = export_dir + f\"layers_to_scores.json\"\n",
    "        hyperparams_path = export_dir + f\"layers_to_hyperparams.json\"\n",
    "        with open(results_path, 'w') as fp:\n",
    "            json.dump(layers_to_results, fp, indent=4)\n",
    "\n",
    "        with open(hyperparams_path, 'w') as fp:\n",
    "            json.dump(layers_to_hyperparams, fp, indent=4)\n",
    "\n",
    "    return layers_to_model, layers_to_results, layers_to_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def repeat_experiment(params, num_runs=5):\n",
    "    all_runs = [ train_and_test(params, layers=TRAIN_LAYERS, \n",
    "                                report_per_period=params['epochs']+1,\n",
    "                                print_results=False, \n",
    "                                export_results=False)\n",
    "                     for _ in range(num_runs) ]\n",
    "    all_run_results = {}  # num layers : list of results across all runs\n",
    "    all_run_hyperparams = {}\n",
    "    for n in TRAIN_LAYERS:\n",
    "        all_run_results[n] = [run[1][n] for run in all_runs]\n",
    "        all_run_hyperparams[n] = [run[2][n] for run in all_runs]\n",
    "        \n",
    "    avg_results = {n : EvalResults.average(results, std=True)\n",
    "                   for (n, results) in all_run_results.items()}\n",
    "    return avg_results, all_run_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TRAINING_PARAMS = {\n",
    "    \"lr\": 0.01,  # learning rate\n",
    "    \"weight_decay\": 0.0005,  # weight_decay\n",
    "    \"epochs\": 1000,  # number of total training epochs\n",
    "    \"max_patience\": 5, # number of k for early stopping\n",
    "    \"hid_dim\": 64 # size of hidden features\n",
    "}\n",
    "\n",
    "def training_params(model_name, dataset_name):\n",
    "    params = DEFAULT_TRAINING_PARAMS.copy()\n",
    "    params['model_name'] = model_name\n",
    "    params['dataset'] = dataset_name\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_results = { model: repeat_experiment(training_params(model, 'Cora'), num_runs=5) \n",
    "                 for model in MODELS }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type float32 is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./results/Cora/experimental_results.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m----> 2\u001b[0m     json\u001b[38;5;241m.\u001b[39mdump(cora_results, fp, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    180\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(chunk)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:432\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:326\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 326\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:326\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 326\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:406\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 406\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:439\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    438\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[0;32m--> 439\u001b[0m o \u001b[38;5;241m=\u001b[39m _default(o)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/json/encoder.py:180\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[1;32m    162\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    181\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Object of type float32 is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# with open('./results/Cora/experimental_results.json', 'w') as fp:\n",
    "#     json.dump(cora_results, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_results = { model: repeat_experiment(training_params(model, 'PubMed'), num_runs=5) \n",
    "                   for model in MODELS }\n",
    "citeseer_results = { model: repeat_experiment(training_params(model, 'CiteSeer'), num_runs=5) \n",
    "                     for model in MODELS }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./results/PubMed/experimental_results.json', 'w') as fp:\n",
    "    json.dump(pubmed_results, fp, indent=4)\n",
    "with open('./results/CiteSeer/experimental_results.json', 'w') as fp:\n",
    "    json.dump(citeseer_results, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: [{'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 2,\n",
       "   'init_res_weight': 0.30000000000000004,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 2,\n",
       "   'init_res_weight': 0.4,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 2,\n",
       "   'init_res_weight': 0.30000000000000004,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 2,\n",
       "   'init_res_weight': 0.30000000000000004,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 2,\n",
       "   'init_res_weight': 0.1,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703}],\n",
       " 5: [{'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 5,\n",
       "   'init_res_weight': 0.4,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 5,\n",
       "   'init_res_weight': 0.8,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 5,\n",
       "   'init_res_weight': 0.5,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 5,\n",
       "   'init_res_weight': 0.8,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 5,\n",
       "   'init_res_weight': 0.2,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703}],\n",
       " 8: [{'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 8,\n",
       "   'init_res_weight': 0.8,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 8,\n",
       "   'init_res_weight': 0.30000000000000004,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 8,\n",
       "   'init_res_weight': 0.7000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 8,\n",
       "   'init_res_weight': 0.9,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 8,\n",
       "   'init_res_weight': 0.7000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703}],\n",
       " 11: [{'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 11,\n",
       "   'init_res_weight': 0.7000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 11,\n",
       "   'init_res_weight': 0.6000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 11,\n",
       "   'init_res_weight': 0.7000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 11,\n",
       "   'init_res_weight': 0.8,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 11,\n",
       "   'init_res_weight': 0.7000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703}],\n",
       " 14: [{'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 14,\n",
       "   'init_res_weight': 0.8,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 14,\n",
       "   'init_res_weight': 0.7000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 14,\n",
       "   'init_res_weight': 0.6000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 14,\n",
       "   'init_res_weight': 0.7000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 14,\n",
       "   'init_res_weight': 0.8,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703}],\n",
       " 17: [{'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 17,\n",
       "   'init_res_weight': 0.6000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 17,\n",
       "   'init_res_weight': 0.5,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 17,\n",
       "   'init_res_weight': 0.5,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 17,\n",
       "   'init_res_weight': 0.6000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 17,\n",
       "   'init_res_weight': 0.8,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703}],\n",
       " 20: [{'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 20,\n",
       "   'init_res_weight': 0.5,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 20,\n",
       "   'init_res_weight': 0.30000000000000004,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 20,\n",
       "   'init_res_weight': 0.6000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 20,\n",
       "   'init_res_weight': 0.30000000000000004,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 20,\n",
       "   'init_res_weight': 0.9,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703}]}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "citeseer_results['WSkipGNN'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hyperparams(experiment_results):\n",
    "    hyperparams = { model: res[1] for (model, res) in experiment_results.items() }\n",
    "    # take the first hyperparams setting for each n-layer-model\n",
    "    model_to_hyperparams = { model: {n: hyperparams[model][n][0] for n in TRAIN_LAYERS} for model in MODELS }\n",
    "    return model_to_hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PlainGNN': {2: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'PlainGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 2,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  5: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'PlainGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 5,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  8: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'PlainGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 8,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  11: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'PlainGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 11,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  14: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'PlainGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 14,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  17: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'PlainGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 17,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  20: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'PlainGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 20,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703}},\n",
       " 'SkipGNN': {2: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'SkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 2,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  5: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'SkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 5,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  8: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'SkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 8,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  11: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'SkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 11,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  14: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'SkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 14,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  17: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'SkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 17,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  20: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'SkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 20,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703}},\n",
       " 'DropGNN': {2: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'DropGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 2,\n",
       "   'dropout_ratio': 0.7000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  5: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'DropGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 5,\n",
       "   'dropout_ratio': 0.4,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  8: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'DropGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 8,\n",
       "   'dropout_ratio': 0.2,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  11: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'DropGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 11,\n",
       "   'dropout_ratio': 0.6000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  14: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'DropGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 14,\n",
       "   'dropout_ratio': 0.30000000000000004,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  17: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'DropGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 17,\n",
       "   'dropout_ratio': 0.6000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  20: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'DropGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 20,\n",
       "   'dropout_ratio': 0.6000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703}},\n",
       " 'JumpKGNN': {2: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'JumpKGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 2,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  5: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'JumpKGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 5,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  8: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'JumpKGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 8,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  11: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'JumpKGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 11,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  14: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'JumpKGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 14,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  17: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'JumpKGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 17,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  20: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'JumpKGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 20,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703}},\n",
       " 'WSkipGNN': {2: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 2,\n",
       "   'init_res_weight': 0.30000000000000004,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  5: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 5,\n",
       "   'init_res_weight': 0.4,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  8: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 8,\n",
       "   'init_res_weight': 0.8,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  11: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 11,\n",
       "   'init_res_weight': 0.7000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  14: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 14,\n",
       "   'init_res_weight': 0.8,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  17: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 17,\n",
       "   'init_res_weight': 0.6000000000000001,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703},\n",
       "  20: {'lr': 0.01,\n",
       "   'weight_decay': 0.0005,\n",
       "   'epochs': 1000,\n",
       "   'max_patience': 5,\n",
       "   'hid_dim': 64,\n",
       "   'model_name': 'WSkipGNN',\n",
       "   'dataset': 'CiteSeer',\n",
       "   'n_layers': 20,\n",
       "   'init_res_weight': 0.5,\n",
       "   'n_classes': 6,\n",
       "   'input_dim': 3703}}}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_hyperparams(citeseer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_acc_vs_nlayers(experimental_results, dataset_name):  # model name to avg scores\n",
    "    # model : { num layers : (avg results, std results) }\n",
    "    model_to_results = { model: res[0] for (model, res) in experimental_results.items() }\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Plot the average accuracy for each class of model\n",
    "    for model_name, results in model_to_results.items():\n",
    "        avg_accs = [results[n][0]['accuracy'] for n in TRAIN_LAYERS]\n",
    "        std_accs = [results[n][1]['accuracy'] for n in TRAIN_LAYERS]\n",
    "        plt.errorbar(TRAIN_LAYERS, avg_accs, yerr=std_accs,\n",
    "                     label=model_name, fmt='o', capsize=5, linestyle='-')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('Number of Layers')\n",
    "    plt.ylabel('Average Accuracy')\n",
    "    plt.title('Average Accuracy over Different Number of Layers')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'./results/{dataset_name}/acc_vs_nlayers.png', format='png')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc_vs_nlayers(cora_results, 'Cora')\n",
    "plot_acc_vs_nlayers(pubmed_results, 'PubMed')\n",
    "plot_acc_vs_nlayers(citeseer_results, 'CiteSeer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vV9IKjZIBUvE"
   },
   "source": [
    "## Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sample_results(dataset_name, experiment_results, \n",
    "                       export_results=True):\n",
    "    '''\n",
    "    Returns sample results as a dictionary of \n",
    "    model : { num layers : (layers_to_model, layers_to_results, layers_to_hyperparams) }\n",
    "    for all models.\n",
    "    '''\n",
    "    model_to_hyperparams = extract_hyperparams(experiment_results)\n",
    "    flatten_single_dict = (lambda triple, n: (triple[0][n], triple[1][n], triple[2][n]))\n",
    "    sample_results = {model: { n: flatten_single_dict(\n",
    "                                        train_and_test(layers_to_params[n], layers=[n], \n",
    "                                                       print_results=False, \n",
    "                                                       export_results=export_results), n)\n",
    "                              for n in TRAIN_LAYERS }\n",
    "                      for model, layers_to_params in model_to_hyperparams.items()}\n",
    "    return sample_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate reduced embeddings for each model and save them in a dictionary object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_embeddings_for_all_models(experiment_results, dataset_name):\n",
    "    sample_results = gen_sample_results(dataset_name, experiment_results, export_results=False)\n",
    "    for model_name, layers_to_results in sample_results.items():\n",
    "        models = { n: res[0] for n, res in layers_to_results.items() }\n",
    "        visualise_embeddings(models, model_name, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualise_embeddings_for_all_models(cora_results, 'Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualise_embeddings_for_all_models(pubmed_results, 'PubMed')\n",
    "visualise_embeddings_for_all_models(citeseer_results, 'CiteSeer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
