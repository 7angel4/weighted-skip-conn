{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a3e3ef-0c56-4dc5-aafb-6a43ce0ba6ad",
   "metadata": {},
   "source": [
    "# Models\n",
    "Here we set up graph convolutional networks for node classification.\n",
    "All our GCN class takes five inputs: *input_dim*, *hid_dim*, *n_class*, *n_layers*\n",
    "\n",
    "- The `forward` function should return a Tensor object: **logits**\n",
    "- The `generate_node_embeddings` fuction should return a Tensor object: **node_embeddings**, which is the representation of the last layer.\n",
    "- We assume all models have at least 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc5284ef-b50c-4c9f-b9c1-1c5aceee3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import typing\n",
    "from typing import Callable\n",
    "import torch_geometric\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.datasets as datasets\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c37c9d98-e4d9-4b6c-9675-180e914fb468",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS = ['PlainGNN', 'SkipGNN', 'JumpKGNN', 'WSkipGNN']\n",
    "MODEL_SPEC_HYPERPARAM = { 'WSkipGNN': 'init_res_weight' }\n",
    "MODEL_HYPERPARAM_RANGE = { 'WSkipGNN': np.arange(-1,1.1,0.2) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84fdccb-cb1c-4234-a6ef-f2761a274b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainGNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hid_dim: int,\n",
    "                 n_classes: int, n_layers: int,\n",
    "                 act_fn: Callable[[torch.Tensor], torch.Tensor] = F.relu):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: input feature dimension\n",
    "            hid_dim: hidden feature dimension\n",
    "            n_classes: number of target classes\n",
    "            n_layers: number of layers\n",
    "        \"\"\"\n",
    "        super(PlainGNN, self).__init__()\n",
    "        # assert n_layers > 1\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes) # final MLP for generating logits\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "        self.param_init()\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        X = self._forward_before_final_layer(X, A)\n",
    "        return self.mlp(X)  # do not apply non-linearity before MLP\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        \"\"\" Generate node embeddings without applying the MLP. \"\"\"\n",
    "        X = self._forward_before_final_layer(X, A)\n",
    "        X = self.act_fn(X)\n",
    "        return X  # raw GNN output without applying MLP\n",
    "\n",
    "    def _forward_before_final_layer(self, X, A) -> torch.Tensor:\n",
    "        \"\"\" Apply all layers except for the final layer and MLP. \"\"\"\n",
    "        for l in self.layers[:-1]: # message-passing through all layers except for the last\n",
    "            X = l(X, A)\n",
    "            X = self.act_fn(X)\n",
    "        return self.layers[-1](X, A)  # raw GNN output without applying MLP\n",
    "\n",
    "    def param_init(self):\n",
    "        # initialise MLP parameters\n",
    "        nn.init.xavier_uniform_(self.mlp.weight)\n",
    "        nn.init.zeros_(self.mlp.bias)\n",
    "        for conv in self.layers:\n",
    "            # initialise weight in each layer's Linear object\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d2e7db2-3a51-4e78-8301-6359d6b827cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hid_dim: int,\n",
    "                 n_classes: int, n_layers: int,\n",
    "                 act_fn: Callable[[torch.Tensor], torch.Tensor] = F.relu):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_dim: input feature dimension\n",
    "          hid_dim: hidden feature dimension\n",
    "          n_classes: number of target classes\n",
    "          n_layers: number of layers\n",
    "          act_fn: activation function\n",
    "        \"\"\"\n",
    "        super(SkipGNN, self).__init__()\n",
    "        assert n_layers > 1\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes)\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "        self.param_init()\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        X = self._forward_before_final_layer(X, A)\n",
    "        return self.mlp(X)\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        \"\"\"Generate node embeddings without applying the MLP.\"\"\"\n",
    "        X = self._forward_before_final_layer(X, A)\n",
    "        return self.act_fn(X)\n",
    "\n",
    "    def _forward_before_final_layer(self, X, A) -> torch.Tensor:\n",
    "        \"\"\" Forward through all layers, \n",
    "            without applying the activation and MLP after the final layer. \n",
    "        \"\"\"\n",
    "        X = self.layers[0](X, A)\n",
    "        X = self.act_fn(X)\n",
    "\n",
    "        for l in self.layers[1:-1]:\n",
    "            residual = X\n",
    "            X = l(X, A)\n",
    "            X = self.act_fn(X)\n",
    "            X = X + residual  # add skip connection\n",
    "\n",
    "        return self.layers[-1](X, A)\n",
    "\n",
    "    def param_init(self):\n",
    "        nn.init.xavier_uniform_(self.mlp.weight)\n",
    "        nn.init.zeros_(self.mlp.bias)\n",
    "        for conv in self.layers:\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92677214-47d7-4577-a54f-29ce9a50043a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JumpKGNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hid_dim: int,\n",
    "                 n_classes: int, n_layers: int,\n",
    "                 act_fn: Callable[[torch.Tensor], torch.Tensor] = F.relu):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: input feature dimension\n",
    "            hid_dim: hidden feature dimension\n",
    "            n_classes: number of target classes\n",
    "            n_layers: number of layers\n",
    "            act_fn: activation function\n",
    "        \"\"\"\n",
    "        super(JumpKGNN, self).__init__()\n",
    "        assert n_layers > 1\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes)\n",
    "        self.act_fn = act_fn\n",
    "\n",
    "        self.param_init()\n",
    "\n",
    "\n",
    "    def _layer_outputs(self, X, A) -> torch.Tensor:\n",
    "        \"\"\" Outputs of all layers\n",
    "            (no activation applied to the final layer -\n",
    "            i.e. last element is just logits)\n",
    "        \"\"\"\n",
    "        outputs = []\n",
    "        for l in self.layers[:-1]:\n",
    "            X = l(X, A)\n",
    "            X = self.act_fn(X)\n",
    "            outputs.append(X)\n",
    "\n",
    "        outputs.append(self.layers[-1](X, A))\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "      return self._layer_outputs(X, A)[-1]\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        outputs = self._layer_outputs(X, A)[:-1]\n",
    "        return torch.max(torch.stack(outputs), 0)[0]  # max pooling\n",
    "\n",
    "    def param_init(self):\n",
    "        for conv in self.layers:\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "457e5126-82a5-4a71-a3a2-971e9050815d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSkipGNN(nn.Module):\n",
    "    def __init__(self, input_dim: int, hid_dim: int,\n",
    "                 n_classes: int, n_layers: int,\n",
    "                 init_res_weight: float = 0.3,\n",
    "                 act_fn: Callable[[torch.Tensor], torch.Tensor] = F.relu):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          input_dim: input feature dimension\n",
    "          hid_dim: hidden feature dimension\n",
    "          n_classes: number of target classes\n",
    "          n_layers: number of layers\n",
    "        \"\"\"\n",
    "        super(WSkipGNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.res_weight = nn.Parameter(torch.tensor(init_res_weight,\n",
    "                                                    dtype=torch.float32))\n",
    "\n",
    "        layers = [GCNConv(input_dim, hid_dim)]\n",
    "        layers += [GCNConv(hid_dim, hid_dim) for _ in range(1, n_layers)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.mlp = nn.Linear(hid_dim, n_classes)\n",
    "        self.act_fn = act_fn\n",
    "        self.param_init()\n",
    "\n",
    "    def forward(self, X, A) -> torch.Tensor:\n",
    "        X = self._forward_before_final_layer(X, A)\n",
    "        return self.mlp(X)  # MLP maps to logits\n",
    "\n",
    "    def generate_node_embeddings(self, X, A) -> torch.Tensor:\n",
    "        \"\"\"Generate node embeddings without applying the MLP.\"\"\"\n",
    "        X = self._forward_before_final_layer(X, A)\n",
    "        return self.act_fn(X)\n",
    "\n",
    "    def _forward_before_final_layer(self, X, A) -> torch.Tensor:\n",
    "        \"\"\" Forward through all layers, \n",
    "            without applying the activation and MLP after the final layer. \n",
    "        \"\"\"\n",
    "        X = self.layers[0](X, A)\n",
    "        X = self.act_fn(X)\n",
    "\n",
    "        for l in self.layers[1:-1]:\n",
    "            residual = X  # previous layer's representation\n",
    "            X = l(X, A)\n",
    "            X = self.act_fn(X)\n",
    "            X = X + self.res_weight * residual  # Add weighted residual connection\n",
    "\n",
    "        return self.layers[-1](X, A)\n",
    "\n",
    "    def param_init(self):\n",
    "        nn.init.xavier_uniform_(self.mlp.weight)\n",
    "        nn.init.zeros_(self.mlp.bias)\n",
    "        for conv in self.layers:\n",
    "            nn.init.xavier_uniform_(conv.lin.weight)\n",
    "            nn.init.zeros_(conv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9daed07d-f469-4125-b824-d2218f943623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_model(params, device):\n",
    "    \"\"\"\n",
    "    Returns the model initialised based on the configuration specified by `params`.\n",
    "    \"\"\"\n",
    "    model_name = params['model_name']\n",
    "    model_class = globals().get(model_name)\n",
    "\n",
    "    if model_class is None or model_name not in MODELS:\n",
    "        raise NotImplementedError(f\"Model '{model_name}' is not implemented.\")\n",
    "\n",
    "    # Dynamically pass only the relevant arguments by filtering params\n",
    "    required_keys = model_class.__init__.__annotations__.keys()\n",
    "    model_args = {key: params[key] for key in required_keys if key in params}\n",
    "\n",
    "    return model_class(**model_args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2373ca4-a132-4b83-9b79-f25c1f65a292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
